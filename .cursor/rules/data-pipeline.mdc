---
description: 
globs: 
alwaysApply: true
---
# Data Pipeline & Ingestion

## Data Processing Pipeline

The data processing pipeline handles ingestion, normalization, and analysis of financial transactions from multiple sources.

### Pipeline Flow
1. **Data Ingestion** → [app/ingestion/loader.py](mdc:app/ingestion/loader.py)
2. **Data Normalization** → [app/ingestion/normalization.py](mdc:app/ingestion/normalization.py)  
3. **Price Data Management** → [app/services/price_service.py](mdc:app/services/price_service.py)
4. **Analytics Processing** → [app/analytics/portfolio.py](mdc:app/analytics/portfolio.py)

## Input Data Sources

### Supported Exchanges
Place transaction CSV files in the `data/` directory:
- `binanceus_transaction_history.csv` - Binance US transactions
- `coinbase_transaction_history.csv` - Coinbase transactions  
- `gemini_staking_transaction_history.csv` - Gemini staking rewards
- `gemini_transaction_history.csv` - Gemini transactions

### Historical Price Data
Format: `data/historical_price_data/historical_price_data_daily_[source]_[symbol]USD.csv`

## Data Normalization

### Core Normalization Module
- [app/ingestion/normalization.py](mdc:app/ingestion/normalization.py) - Standardizes transaction schemas
  - Normalizes transaction types across exchanges
  - Handles currency symbol mapping
  - Maps institution-specific fields to unified schema
  - Processes internal transfers between accounts

### Unified Transaction Schema
```python
REQUIRED_COLUMNS = [
    'timestamp',    # datetime
    'type',        # string (buy, sell, transfer_in, transfer_out, etc.)
    'asset',       # string (BTC, ETH, etc.)
    'amount',      # float (quantity of asset)
    'price',       # float (price per unit in USD)
    'fees',        # float (transaction fees, optional)
    'account_id',  # string (exchange/account identifier)
    'source'       # string (data source identifier)
]
```

### Transaction Type Mapping
```python
TRANSACTION_TYPE_MAPPING = {
    # Binance US
    'Buy': 'buy',
    'Sell': 'sell', 
    'Deposit': 'transfer_in',
    'Withdraw': 'transfer_out',
    
    # Coinbase
    'Buy': 'buy',
    'Sell': 'sell',
    'Receive': 'transfer_in',
    'Send': 'transfer_out',
    
    # Gemini
    'Buy': 'buy',
    'Sell': 'sell',
    'Deposit': 'transfer_in',
    'Withdrawal': 'transfer_out',
    'Credit': 'staking_reward'
}
```

## Database Migration

### Migration Process
- [migration.py](mdc:migration.py) - Main migration script
  - Creates database schema from [schema.sql](mdc:schema.sql)
  - Imports normalized transaction data
  - Loads historical price data
  - Handles data validation and integrity checks

### Database Schema
- [schema.sql](mdc:schema.sql) - Complete database schema
  - `assets` - Asset metadata (symbol, name, type)
  - `accounts` - Account information by exchange
  - `price_data` - Historical price data with source tracking
  - `position_daily` - Daily position snapshots
  - Proper indexes for performance

## Output Files

Generated in the `output/` directory:
- `transactions_normalized.csv` - Unified transaction ledger
- `portfolio_timeseries.csv` - Portfolio value over time
- `cost_basis_fifo.csv` - FIFO cost basis calculations
- `cost_basis_avg.csv` - Average cost basis calculations
- `performance_report.csv` - Portfolio performance metrics

## Data Validation Patterns

### Input Validation
```python
def validate_transaction_data(df: pd.DataFrame) -> bool:
    """Validate transaction data structure and content."""
    required_columns = ['timestamp', 'type', 'asset', 'amount', 'price']
    
    # Check required columns
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"Missing required columns: {missing_columns}")
    
    # Validate data types
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
    df['price'] = pd.to_numeric(df['price'], errors='coerce')
    
    # Check for null values in critical columns
    if df[required_columns].isnull().any().any():
        raise ValueError("Null values found in required columns")
    
    return True
```

### Data Quality Checks
```python
def perform_data_quality_checks(transactions: pd.DataFrame) -> Dict[str, Any]:
    """Perform comprehensive data quality checks."""
    checks = {
        'total_transactions': len(transactions),
        'date_range': (transactions['timestamp'].min(), transactions['timestamp'].max()),
        'unique_assets': transactions['asset'].nunique(),
        'transaction_types': transactions['type'].value_counts().to_dict(),
        'missing_prices': transactions['price'].isnull().sum(),
        'zero_amounts': (transactions['amount'] == 0).sum()
    }
    return checks
```

## Error Handling

### Graceful Degradation
```python
def load_and_normalize_data(file_path: str) -> Optional[pd.DataFrame]:
    """Load and normalize transaction data with error handling."""
    try:
        # Load raw data
        raw_data = pd.read_csv(file_path)
        
        # Normalize data
        normalized_data = normalize_transactions(raw_data)
        
        # Validate result
        validate_transaction_data(normalized_data)
        
        return normalized_data
        
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        return None
    except ValueError as e:
        logger.error(f"Data validation error: {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error processing {file_path}: {e}")
        return None
```

## Performance Optimization

### Efficient Data Loading
```python
# Use efficient pandas operations
df = pd.read_csv(file_path, 
                 parse_dates=['timestamp'],
                 dtype={'amount': 'float64', 'price': 'float64'})

# Vectorized operations for normalization
df['normalized_type'] = df['type'].map(TRANSACTION_TYPE_MAPPING)
```

### Memory Management
```python
# Process large files in chunks
chunk_size = 10000
for chunk in pd.read_csv(file_path, chunksize=chunk_size):
    processed_chunk = normalize_transactions(chunk)
    # Process chunk
```

## Integration Points

### Price Service Integration
- [app/services/price_service.py](mdc:app/services/price_service.py) - Price data management
  - Retrieves historical prices for portfolio valuation
  - Handles multiple price data sources
  - Caches price data for performance

### Analytics Integration
- [app/analytics/portfolio.py](mdc:app/analytics/portfolio.py) - Portfolio analytics
  - Consumes normalized transaction data
  - Computes cost basis and performance metrics
  - Generates tax-relevant calculations

## Common Data Issues

### Missing Amount Column
**Problem**: Dashboard expects `amount` column but CSV has `quantity`
**Solution**: 
```python
# Add missing amount column
if 'amount' not in df.columns and 'quantity' in df.columns:
    df['amount'] = df['quantity']
```

### Inconsistent Date Formats
**Problem**: Different exchanges use different date formats
**Solution**:
```python
# Standardize date parsing
df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)
```

### Currency Symbol Variations
**Problem**: Same asset with different symbols (BTC vs Bitcoin)
**Solution**:
```python
SYMBOL_MAPPING = {
    'Bitcoin': 'BTC',
    'Ethereum': 'ETH',
    'USD Coin': 'USDC'
}
df['asset'] = df['asset'].map(SYMBOL_MAPPING).fillna(df['asset'])
```
